% METODOLOGIA EXPERIMENTAL

% Texto principal.

% Texto principal. Para cada trabalho experimental deve descrever inicialmente o aparato experimental utilizado – com diagrama de blocos resumido do sistema desenvolvido (uma figura bem apresentada ou foto coerente com a descrição dos itens que formam o aparato experimental). Depois abrir em subcapítulos e descrever os principais itens do experimento desenvolvido ou projeto desenvolvido. \textbf{Este capítulo não deve conter resultados} e sim a descrição do método/experimento desenvolvido. De forma geral e simples este capítulo deve apresentar “como foi feito” o experimento e evidentemente para isso deve descrever todo e qualquer item utilizado no experimento (cuidado apenas para não tornar o relatório infantil e “sem fundamentação científica” nos experimentos simples principalmente).

% Toda e qualquer figura e tabela deve conter a respectiva chamada no texto, por exemplo, a Figura~\ref{diagrama_blocos} apresenta o diagrama de blocos do experimento desenvolvido para calibrar...etc. A Tabela \ref{tabela_tabuadas} apresenta os dados....etc. Equações também devem seguir a seguinte formatação (utilizar um bom editor de equações) e as mesmas devem ser numeradas sequencialmente no texto. Por exemplo:

% % No parágrafo acima, o trecho Figura~\ref{diagrama_blocos} está com um ~ antes da referência. Isso faz com que a palavra antecedente (Figura) não fique separada em 2 linhas, caso não haja espaço na mesma linha. Nesse caso, isso acontecerá se o ~ for removido.

% \begin{equation}
%     V = R * i
% \label{equação_ohm}
% \end{equation}

% Por exemplo, a Equação \ref{equação_ohm} representa a...etc.... onde V representa a tensão elétrica [V], R a resistência elétrica [Ω] e i....

% Texto principal. Todo e qualquer circuito devem conter a respectiva equação de medição....

% Texto principal. Programas desenvolvidos devem apresentar o fluxograma da rotina e a descrição dos principais blocos ou funções desenvolvidas e/ou utilizadas no corpo principal do texto (o restante pode ser colocado em um apêndice no final do relatório – não esquecer de apresentar a respectiva chamada, como por exemplo, a rotina principal do....bláblá.....encontra-se no Apêndice \ref{código_simples}. A Figura \ref{fluxograma_N} apresenta o fluxograma representando a rotina para ...... e descrever os principais blocos.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.5\textwidth]{Figuras/figura_exemplo.png}
%     \caption{Fluxograma representando a rotina desenvolvida no (...).\\\textbf{Fonte -} Fonte da figura.}
%     \label{fluxograma_N}
% \end{figure}

% \subsection{Sub-capítulo}
% Texto principal.

% Texto principal.

% \subsection{Sub-capítulo}
% Texto principal.

% Texto principal.

%%%%%%%%%%%%%%%%%%%%%%% Acima  Modelo %%%%%%%%%%%%%%%%%%%%%%%%



%%\subsection{Definição das especificações do instrumento}
% A fim de aplicar os conceitos relacionados ao desenvolvimento de Sistemas Especialistas com uso de lógica Fuzzy, são explorados $4$ projetos, cujas propostas e análises dos tratamentos são descrito a seguir.

A fim de aplicar os conceitos relacionados ao desenvolvimento de \textit{Clusters}, são explorados $5$ exercícios, cujas propostas e análises dos tratamentos são descrito a seguir.

% Os dados coletados foram

% \subsection{Sistema fuzzy para automatizar um cultivo hidropônico de vegetais}

\subsection{Exercício i}

Foi selecionada a base de dados  \href{https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+\%28Diagnostic\%29}{Breast Cancer Wisconsin (Diagnostic) Data Set} \citep{wolberg_uci_1995}
 disponível em \url{https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+\%28Diagnostic\%29}.

Esse conjunto de dados da área oncológica contempla análise de células tumorais com base em imagens digitalizadas. As características, $32$, são obtidas a partir de uma imagem digitalizada de uma Punção aspirativa por agulha fina (PAAF, do inglês \textit{Fine-needle aspiration} - FNA) de uma massa mamária. Eles descrevem as características dos núcleos celulares presentes na imagem.

% contempla medições do consumo de energia elétrica em uma residência com uma taxa de amostragem de um minuto ao longo de um período de quase $4$ anos. Diferentes quantidades elétricas e alguns valores de sub-medição estão disponíveis.

Este arquivo contém $569$ medidas reunidas em uma casa localizada em Sceaux ($7$ km de Paris, França) entre dezembro de $2006$ e novembro de $2010$ ($47$ meses). A Tabela \ref{tab:individual-household-electric-power-consumption} reune algumas informações sobre o conjunto de dados.

\input{Tabelas/Diagnostic-Wisconsin-Breast-Cancer-Database}

% Observações:
% \begin{enumerate}
%     \item (global\_active\_power $\cdot 1000/60$ - sub\_metering\_1 - sub\_metering\_2 - sub\_metering\_3) representa a energia ativa consumida a cada minuto (em watt hora) na casa por equipamentos elétricos não medidos nos sub-medidores 1, 2 e 3.
    
%     \item O conjunto de dados contém alguns valores ausentes nas medições (quase $1,25\%$ das linhas). Todos os carimbos de tempo do calendário estão presentes no conjunto de dados, mas para alguns carimbos de tempo, os valores de medição estão faltando: um valor faltante é representado pela ausência de valor entre dois separadores consecutivos de atributos ponto-e-vírgula. Por exemplo, o conjunto de dados mostra valores ausentes em $28$ de abril de $2007$.
% \end{enumerate}

Informação dos Atributos:

Cada imagem possui um número de identificação e um diagnóstico, sendo $M$ para maligno e $B$ para benigno. Os outros $30$ atributros estão relacionados a dez características de valor real que são computadas para cada núcleo de célula:

\begin{enumerate}
    \item \textit{radius} - raio (média das distâncias do centro aos pontos do perímetro);
	\item \textit{texture} - textura (desvio padrão dos valores da escala de cinza);
	\item \textit{perimeter} - perímetro;
	\item \textit{area} - área;
	\item \textit{smoothness} - suavidade (variação local em comprimentos de raio);
	\item \textit{compactness} - compacidade ($\text{perímetro}^2$ / área - $1,0$);
	\item \textit{concavity} - concavidade (gravidade das porções côncavas do contorno);
	\item \textit{concave points} - pontos côncavos (número de porções côncavas do contorno);
	\item \textit{symmetry} - simetria ;
	\item \textit{fractal dimension} - dimensão fractal ("aproximação da linha de costa" - $1$).
\end{enumerate}

\cite{street_nuclear_1999} aponta que o valor médio (\textit{mean value}), maior valor (\textit{extreme, largest value} e erro padrão (\textit{standard error}) são determinados para cada imagem, sendo os valores extremos os mais intuitivamente úteis, uma vez que apenas algumas poucas células malignas pode ocorrer em uma determinada amostra.

Todas as características foram registradas com $4$ dígitos significativos. Não há valores ausentes e a classe é desbalanceada. Há $357$ tumores benignos e $212$ malignos.

\subsubsection{Análise do conjunto de dados coletado}

Para esta análise foi utilizada a linguagem de programação Python v$3.7.13$ na plataforma Google Colab. As bibliotecas  presentes na Tabela \ref{tab: bibliotecas-python} foram utilizadas:

\input{Tabelas/bibliotecas-python}

Inicialmente, foram carregados os dados através da função \textit{read\_csv} do módulo \textit{pandas} em uma estrutura DataFrame deste. Neste processo, foram fusionadas as colunas 'Date' e 'Time' do conjunto de dados original. Essa nova coluna foi utilizada como index do DataFrame.  

Conforme \href{pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html}{pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to\_datetime.html}, definir o parâmetro \textit{infer\_datatime\_format} como verdadeiro pode aumentar a velocidade da etapa de \textit{parsing} entre $5$ à $10$ vezes, de modo que esse parametro foi setado.

Os pontos de '?' presentes no dataset para o caso de ausência de dados foram transformados em \textit{numpy.nan}, afim de que todas os elementos fossem de um único tipo (ponto flutuante, no caso).

Então, uma nova coluna foi criada, Sub\_metering\_4, correspondente  a energia ativa consumida a cada minuto (em watt hora) na casa por equipamentos elétricos não medidos nos sub-medidores 1, 2 e 3.

Esse novo dataset foi salvo como o nome "household\_power\_consumption.csv" e foi utilizado na sequência em um novo DataFrame.

Utilizando a função \textit{pandas.DataFrame.resample} com os parâmetros 'H' e 'D', dois novos DataFrames foram criados baseados na média dos valores horários e diários.

Então, foram plotadas os gráficos das variáveis em função do tempo para o DataFrame com intervalo por minuto e para os dois novos DataFrames.

Também foram criados os histogramas e um mapa de correlações entre cada uma das variáveis, afim de determinar quais poderiam ser utilizadas na etapa de agregação.

Selecionadas as características, e tendo sido selecionado o intervalo horário, como utilizado por \cite{jain_forecasting_2014} , procedeu-se a normalização dos dados entre 0 e 1. As linhas não utilizadas anteriormente, contendo \textit{numpy.nan}, foram descartas considerando-se que  correspondiam a um percentual pequeno da base de dados.

A métrica de distância foi definida como a distância euclidiana, como em \cite{beckel_revealing_cluster_2014}.

Neste exercício, o algoritmo de agrupamento k-means foi utilizado. No exercício 11, descrito a seguir, o método de agrupamento k-medoids (PAM) foi utilizado.

O Coeficiente de Silhueta foi então analisado para diversos valores de k, bem como a métrica de dispersão \textit{Within Cluster Sum of Squares}, gerando uma "curva do cotovelo". 

O comportamento obtido, bem como os histogramas elaborados anteriormente e a utilização $5$ clusters por \cite{beckel_revealing_cluster_2014} foram então utilizados para determinar o número de clusters final.

Com os dados classificados, é possível utilizar o método KNN. Esse método pode ser treinado com a técnica k-fold. Os dados  foram separados em um conjunto de teste e um conjunto de treinamento. Houve a divisão dos dados em dez lotes utilizando o recurso de validação cruzada da biblioteca "Scikit Learn". De forma que se pudesse garantir que a pseudo-aleatoriedade não afetasse o teste assim o valor utilizado como referência foi a média de $10$ rodadas de testes. O uso do K-fold é especialmente interessante para a comparação deste exercício $4$ com o exercício $11$.


% Nesta etapa, a métrica Mean absolute percentage error (MAPE) foi utilizada. Está métrica está disponível no módulo \textit{sklearn} 
% \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html}{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean\_absolute\_percentage\_error.html}

Nesta etapa, a métrica "acurracy" \ foi utilizada, ao invés da métrica MAPE, tendo em vista que, para o método K-NN, as classes determinadas anteriormente são utilizadas.


\subsection{Exercício 11}

O Exercício 11 aborda a mesma situação do exercício 4, porém utilizando o método PAM com k-memoids.

Os mesmos módulos utilizados no exercício 4 foram utilidos, com a adição do módulo sklearn\_extra para a utilização do método k-memoids com parâmetro.

O mesmo código foi utilizado, substituindo-se as referências a k-means por K-medoids. O objetivo foi comparar duas versões otimizadas do algoritmos em bibliotecas estabelecidas e seus resultados para a aplicação em questão.

O Coeficiente de Silhueta foi então analisado para diversos valores de k, bem como a métrica de dispersão \textit{Within Cluster Sum of Squares}, gerando uma "curva do cotovelo".

Com os dados classificados, é possível utilizar o método KNN. Esse método pode ser treinado com a técnica k-fold. Os dados  foram separados em um conjunto de teste e um conjunto de treinamento. Houve a divisão dos dados em dez lotes utilizando o recurso de validação cruzada da biblioteca "Scikit Learn". De forma que se pudesse garantir que a pseudo-aleatoriedade não afetasse o teste, o valor utilizado como referência foi a média de $10$ rodadas de testes. O uso do K-fold é especialmente interessante para a comparação dos resultados deste exercício $11$ com o exercício $4$.

Novamente, a métrica "accuracy" \ foi utilizada.

\subsection{Exercício 12: Implementação do k-means}
Uma implementação do k-means com o pseudocódigo visto em aula foi realizada. Foi solicitado buscar o que poderia ser otimizado. O algorítmo foi comparado com as funções do scikit-learn disponíveis na documentação oficial.

\subsection{Exercício 15: Sementes}

O Exercício 15 é feito com base no dataset \href{https://archive.ics.uci.edu/ml/datasets/seeds}{Seeds} \citep{dataset_charytanowicz_uci_2012}. Consiste num conjunto de medidas de 210 sementes de trigo, sendo 70 de cada uma das três variedades: Kama, Rosa e Canadense. Poucos detalhes puderam ser encontrados sobre essas variedades, aparentam ser variedades comerciais de trigo disponíveis no local do estudo. Sete características geométricas foram aferidas: (1) Área A, (2) Perímetro P, (3) Compacidade = $\frac{4\pi A}{P^2}$, (4) Comprimento, (5) Largura, (6) Coeficiente de assimetria e (7) Comprimento da fenda, sendo todos os parâmetros reais e contínuos.

Segundo \citep{charytanowicz} os dados foram obtidos através de imagens de raio-x registradas em placas KODAK de tamanho 13x18cm, que foram escaneadas posteriormente e processadas com o pacote de software GRAINS, desenvolvido especialmente para avaliar grãos do tipo. O método de imagem via raio-x permite analisar o interior do grão e é mais barato que outros métodos como os baseados em laser e microscopia\citep{charytanowicz}.

Partindo da premissa que são três variedades esperadas, o valor k para o algorítimo k-means escolhido foi 3. Executou-se o kmeans com todos os dados geométricos disponíveis, todos os dados geométricos normalizados e por fim escolheu-se as duas características com coeficiente de correlação mais próximo de 0 e a característica da área a fim de avaliar o método com número menor de variáveis.

Assim criou-se 4 conjuntos de clusters e o desempenho foi avaliado com a etiqueta das sementes definida no conjunto de dados usando as métricas \textit{Adjusted Rand Index (ARI)} e \textit{Adjusted Mutual Info}.

\subsection{Exercício 16: Vendas}

O Exercício 16 usa a base de dados Foi selecionada a base de dados  \href{https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly}{Sales{\_}Transactions{\_}Dataset{\_}Weekly} \citep{dataset_tan_uci_2017}.

São valores de 52 semanas, normalizados e não normalizados para 811 produtos. Os valores  foram usados para gerar clusters com produtos através do algoritmo k-means implementado pela biblioteca sklearn. Cada ponto no conjunto é relacionado a um produto, dessa forma ao se agrupar por vendas por semana espera-se encontrar produtos de venda correlacionada e com métricas intrínsecas ao agrupamento tentar encontrar a quantidade de clusters adequada.

Após, colheu-se as métricas de \textit{WCSS} e medida de silhueta média. Traçou-se então as curvas relacionas ao número do valor k e os resultados das métricas.